{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIRTUAL PAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f2147e04e1df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mimgResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mnewPoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyColors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmyColorValues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewPoints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "    import cv2\n",
    "    import numpy as np\n",
    "    frameWidth = 640\n",
    "    frameHeight = 480\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    cap.set(3, frameWidth)\n",
    "    cap.set(4, frameHeight)\n",
    "    cap.set(10,150)\n",
    "\n",
    "    myColors = [[5,107,0,19,255,255],\n",
    "                [133,56,0,159,156,255],\n",
    "                [57,76,0,100,255,255],\n",
    "                [90,48,0,118,255,255]]\n",
    "    myColorValues = [[51,153,255],          ## BGR\n",
    "                     [255,0,255],\n",
    "                     [0,255,0],\n",
    "                     [255,0,0]]\n",
    "\n",
    "    myPoints =  []  ## [x , y , colorId ]\n",
    "\n",
    "    def findColor(img,myColors,myColorValues):\n",
    "        imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        count = 0\n",
    "        newPoints=[]\n",
    "        for color in myColors:\n",
    "            lower = np.array(color[0:3])\n",
    "            upper = np.array(color[3:6])\n",
    "            mask = cv2.inRange(imgHSV,lower,upper)\n",
    "            x,y=getContours(mask)\n",
    "            cv2.circle(imgResult,(x,y),15,myColorValues[count],cv2.FILLED)\n",
    "            if x!=0 and y!=0:\n",
    "                newPoints.append([x,y,count])\n",
    "            count +=1\n",
    "            #cv2.imshow(str(color[0]),mask)\n",
    "        return newPoints\n",
    "\n",
    "    def getContours(img):\n",
    "        contours,hierarchy = cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "        x,y,w,h = 0,0,0,0\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area>500:\n",
    "                #cv2.drawContours(imgResult, cnt, -1, (255, 0, 0), 3)\n",
    "                peri = cv2.arcLength(cnt,True)\n",
    "                approx = cv2.approxPolyDP(cnt,0.02*peri,True)\n",
    "                x, y, w, h = cv2.boundingRect(approx)\n",
    "        return x+w//2,y\n",
    "\n",
    "    def drawOnCanvas(myPoints,myColorValues):\n",
    "        for point in myPoints:\n",
    "            cv2.circle(imgResult, (point[0], point[1]), 10, myColorValues[point[2]], cv2.FILLED)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        imgResult = img.copy()\n",
    "        newPoints = findColor(img, myColors,myColorValues)\n",
    "        if len(newPoints)!=0:\n",
    "            for newP in newPoints:\n",
    "                myPoints.append(newP)\n",
    "        if len(myPoints)!=0:\n",
    "            drawOnCanvas(myPoints,myColorValues)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Result\", imgResult)\n",
    "        if cv2.waitKey(1) and 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-jmqlme5z\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bc525203f9c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mimgHsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2HSV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mh_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrackbarPos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HUE Min\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"HSV\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-jmqlme5z\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    frameWidth = 640\n",
    "    frameHeight = 480\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    cap.set(3, frameWidth)\n",
    "    cap.set(4, frameHeight)\n",
    "\n",
    "\n",
    "    def empty(a):\n",
    "        pass\n",
    "\n",
    "    cv2.namedWindow(\"HSV\")\n",
    "    cv2.resizeWindow(\"HSV\", 640, 240)\n",
    "    cv2.createTrackbar(\"HUE Min\", \"HSV\", 0, 179, empty)\n",
    "    cv2.createTrackbar(\"HUE Max\", \"HSV\", 179, 179, empty)\n",
    "    cv2.createTrackbar(\"SAT Min\", \"HSV\", 0, 255, empty)\n",
    "    cv2.createTrackbar(\"SAT Max\", \"HSV\", 255, 255, empty)\n",
    "    cv2.createTrackbar(\"VALUE Min\", \"HSV\", 0, 255, empty)\n",
    "    cv2.createTrackbar(\"VALUE Max\", \"HSV\", 255, 255, empty)\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        success, img = cap.read()\n",
    "        imgHsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        h_min = cv2.getTrackbarPos(\"HUE Min\", \"HSV\")\n",
    "        h_max = cv2.getTrackbarPos(\"HUE Max\", \"HSV\")\n",
    "        s_min = cv2.getTrackbarPos(\"SAT Min\", \"HSV\")\n",
    "        s_max = cv2.getTrackbarPos(\"SAT Max\", \"HSV\")\n",
    "        v_min = cv2.getTrackbarPos(\"VALUE Min\", \"HSV\")\n",
    "        v_max = cv2.getTrackbarPos(\"VALUE Max\", \"HSV\")\n",
    "        print(h_min)\n",
    "\n",
    "        lower = np.array([h_min, s_min, v_min])\n",
    "        upper = np.array([h_max, s_max, v_max])\n",
    "        mask = cv2.inRange(imgHsv, lower, upper)\n",
    "        result = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "        hStack = np.hstack([img, mask, result])\n",
    "        cv2.imshow('Horizontal Stacking', hStack)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition â€“ Unlock Your Application With Your Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Samples Complete\n"
     ]
    }
   ],
   "source": [
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    # Load HAAR face classifier\n",
    "    face_classifier = cv2.CascadeClassifier(r\"E:\\haarcascades\\haarcascade_frontalface_default.xml\")\n",
    "\n",
    "    # Load functions\n",
    "    def face_extractor(img):\n",
    "        # Function detects faces and returns the cropped face\n",
    "        # If no face detected, it returns the input image\n",
    "\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        if faces is ():\n",
    "            return None\n",
    "\n",
    "        # Crop all faces found\n",
    "        for (x,y,w,h) in faces:\n",
    "            cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "        return cropped_face\n",
    "\n",
    "    # Initialize Webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    count = 0\n",
    "\n",
    "    # Collect 100 samples of your face from webcam input\n",
    "    while True:\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if face_extractor(frame) is not None:\n",
    "            count += 1\n",
    "            face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Save file in specified directory with unique name\n",
    "            file_name_path = './faces/user/' + str(count) + '.jpg'\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "\n",
    "            # Put count on images and display live count\n",
    "            cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Cropper', face)\n",
    "\n",
    "        else:\n",
    "            print(\"Face not found\")\n",
    "            pass\n",
    "\n",
    "        if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()      \n",
    "    print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained sucessefully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Get the training data we previously made\n",
    "data_path = './faces/user/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "# Create arrays for training data and labels\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "# Open training images in our datapath\n",
    "# Create a numpy array for training data\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "\n",
    "# Create a numpy array for both training data and labels\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "# Initialize facial recognizer\n",
    "#model = cv2.face.createLBPHFaceRecognizer()\n",
    "model=cv2.face.LBPHFaceRecognizer_create()\n",
    "# NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "\n",
    "# Let's train our model \n",
    "model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model trained sucessefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Run Our Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dbb1be051139>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-dbb1be051139>\u001b[0m in \u001b[0;36mface_detector\u001b[1;34m(img, size)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Convert image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(r\"E:\\haarcascades\\haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img, []\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi\n",
    "\n",
    "\n",
    "# Open Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    image, face = face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Pass face to prediction model\n",
    "        # \"results\" comprises of a tuple containing the label and the confidence value\n",
    "        results = model.predict(face)\n",
    "        \n",
    "        if results[1] < 500:\n",
    "            confidence = int( 100 * (1 - (results[1])/400) )\n",
    "            display_string = str(confidence) + '% Confident it is User'\n",
    "            \n",
    "        cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "        \n",
    "        if confidence > 75:\n",
    "            cv2.putText(image, \"Unlocked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "        else:\n",
    "            cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "\n",
    "    except:\n",
    "        cv2.putText(image, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Scanner from webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    ###################################\n",
    "    widthImg=540\n",
    "    heightImg =640\n",
    "    #####################################\n",
    "\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    cap.set(10,150)\n",
    "\n",
    "    def preProcessing(img):\n",
    "        imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        imgBlur = cv2.GaussianBlur(imgGray,(5,5),1)\n",
    "        imgCanny = cv2.Canny(imgBlur,200,200)\n",
    "        kernel = np.ones((5,5))\n",
    "        imgDial = cv2.dilate(imgCanny,kernel,iterations=2)\n",
    "        imgThres = cv2.erode(imgDial,kernel,iterations=1)\n",
    "        return imgThres\n",
    "\n",
    "    def getContours(img):\n",
    "        biggest = np.array([])\n",
    "        maxArea = 0\n",
    "        contours,hierarchy = cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area>5000:\n",
    "                #cv2.drawContours(imgContour, cnt, -1, (255, 0, 0), 3)\n",
    "                peri = cv2.arcLength(cnt,True)\n",
    "                approx = cv2.approxPolyDP(cnt,0.02*peri,True)\n",
    "                if area >maxArea and len(approx) == 4:\n",
    "                    biggest = approx\n",
    "                    maxArea = area\n",
    "        cv2.drawContours(imgContour, biggest, -1, (255, 0, 0), 20)\n",
    "        return biggest\n",
    "\n",
    "    def reorder (myPoints):\n",
    "        myPoints = myPoints.reshape((4,2))\n",
    "        myPointsNew = np.zeros((4,1,2),np.int32)\n",
    "        add = myPoints.sum(1)\n",
    "        #print(\"add\", add)\n",
    "        myPointsNew[0] = myPoints[np.argmin(add)]\n",
    "        myPointsNew[3] = myPoints[np.argmax(add)]\n",
    "        diff = np.diff(myPoints,axis=1)\n",
    "        myPointsNew[1]= myPoints[np.argmin(diff)]\n",
    "        myPointsNew[2] = myPoints[np.argmax(diff)]\n",
    "        #print(\"NewPoints\",myPointsNew)\n",
    "        return myPointsNew\n",
    "\n",
    "    def getWarp(img,biggest):\n",
    "        biggest = reorder(biggest)\n",
    "        pts1 = np.float32(biggest)\n",
    "        pts2 = np.float32([[0, 0], [widthImg, 0], [0, heightImg], [widthImg, heightImg]])\n",
    "        matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "        imgOutput = cv2.warpPerspective(img, matrix, (widthImg, heightImg))\n",
    "\n",
    "        imgCropped = imgOutput[20:imgOutput.shape[0]-20,20:imgOutput.shape[1]-20]\n",
    "        imgCropped = cv2.resize(imgCropped,(widthImg,heightImg))\n",
    "\n",
    "        return imgCropped\n",
    "\n",
    "\n",
    "    def stackImages(scale,imgArray):\n",
    "        rows = len(imgArray)\n",
    "        cols = len(imgArray[0])\n",
    "        rowsAvailable = isinstance(imgArray[0], list)\n",
    "        width = imgArray[0][0].shape[1]\n",
    "        height = imgArray[0][0].shape[0]\n",
    "        if rowsAvailable:\n",
    "            for x in range ( 0, rows):\n",
    "                for y in range(0, cols):\n",
    "                    if imgArray[x][y].shape[:2] == imgArray[0][0].shape [:2]:\n",
    "                        imgArray[x][y] = cv2.resize(imgArray[x][y], (0, 0), None, scale, scale)\n",
    "                    else:\n",
    "                        imgArray[x][y] = cv2.resize(imgArray[x][y], (imgArray[0][0].shape[1], imgArray[0][0].shape[0]), None, scale, scale)\n",
    "                    if len(imgArray[x][y].shape) == 2: imgArray[x][y]= cv2.cvtColor( imgArray[x][y], cv2.COLOR_GRAY2BGR)\n",
    "            imageBlank = np.zeros((height, width, 3), np.uint8)\n",
    "            hor = [imageBlank]*rows\n",
    "            hor_con = [imageBlank]*rows\n",
    "            for x in range(0, rows):\n",
    "                hor[x] = np.hstack(imgArray[x])\n",
    "            ver = np.vstack(hor)\n",
    "        else:\n",
    "            for x in range(0, rows):\n",
    "                if imgArray[x].shape[:2] == imgArray[0].shape[:2]:\n",
    "                    imgArray[x] = cv2.resize(imgArray[x], (0, 0), None, scale, scale)\n",
    "                else:\n",
    "                    imgArray[x] = cv2.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None,scale, scale)\n",
    "                if len(imgArray[x].shape) == 2: imgArray[x] = cv2.cvtColor(imgArray[x], cv2.COLOR_GRAY2BGR)\n",
    "            hor= np.hstack(imgArray)\n",
    "            ver = hor\n",
    "        return ver\n",
    "\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        img = cv2.resize(img,(widthImg,heightImg))\n",
    "        imgContour = img.copy()\n",
    "\n",
    "        imgThres = preProcessing(img)\n",
    "        biggest = getContours(imgThres)\n",
    "        if biggest.size !=0:\n",
    "            imgWarped=getWarp(img,biggest)\n",
    "            # imageArray = ([img,imgThres],\n",
    "            #           [imgContour,imgWarped])\n",
    "            imageArray = ([imgContour, imgWarped])\n",
    "            cv2.imshow(\"ImageWarped\", imgWarped)\n",
    "        else:\n",
    "            # imageArray = ([img, imgThres],\n",
    "            #               [img, img])\n",
    "            imageArray = ([imgContour, img])\n",
    "\n",
    "        stackedImages = stackImages(0.6,imageArray)\n",
    "        cv2.imshow(\"WorkFlow\", stackedImages)\n",
    "\n",
    "        if cv2.waitKey(1) and 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number plate detector from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-jmqlme5z\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5bccf70bd828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mimgGray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mnumberPlates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnPlateCascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgGray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnumberPlates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-jmqlme5z\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "    import cv2\n",
    "\n",
    "    #############################################\n",
    "    frameWidth = 640\n",
    "    frameHeight = 480\n",
    "    nPlateCascade = cv2.CascadeClassifier(r\"E:\\haarcascades\\haarcascade_russian_plate_number.xml\")\n",
    "    minArea = 200\n",
    "    color = (255,0,255)\n",
    "    ###############################################\n",
    "\n",
    "    cap = cv2.VideoCapture(r\"E:\\resources\\plates.mp4\")\n",
    "    cap.set(3, frameWidth)\n",
    "    cap.set(4, frameHeight)\n",
    "    cap.set(10,150)\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        success, img = cap.read()\n",
    "        imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        numberPlates = nPlateCascade.detectMultiScale(imgGray, 1.1, 10)\n",
    "        for (x, y, w, h) in numberPlates:\n",
    "                area = w*h\n",
    "                if area >minArea:\n",
    "                    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 2)\n",
    "                    cv2.putText(img,\"Number Plate\",(x,y-5),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX_SMALL,1,color,2)\n",
    "                    imgRoi = img[y:y+h,x:x+w]\n",
    "                    cv2.imshow(\"ROI\", imgRoi)\n",
    "\n",
    "        cv2.imshow(\"Result\", img)\n",
    "\n",
    "        if cv2.waitKey(1) and 0xFF == ord('s'):\n",
    "            cv2.imwrite(r\"E:\\resources\\Scanned\\noplate_\"+str(count)+\".jpg\",imgRoi)\n",
    "            cv2.rectangle(img,(0,150),(540,200),(0,255,0),cv2.FILLED)\n",
    "            cv2.putText(img,\"Scan Saved\",(150,265),cv2.FONT_HERSHEY_DUPLEX,2,(0,0,255),2)\n",
    "            cv2.imshow(\"Result\",img)\n",
    "            cv2.waitKey(500)\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94749016 0.05250988]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "# Disable scientific notation for clarity\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Load the model\n",
    "model = tensorflow.keras.models.load_model(r\"C:\\Users\\ASUS\\Downloads\\converted_keras\\keras_model.h5\",compile=False)\n",
    "\n",
    "# Create the array of the right shape to feed into the keras model\n",
    "# The 'length' or number of images you can put into the array is\n",
    "# determined by the first position in the shape tuple, in this case 1.\n",
    "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "# Replace this with the path to your image\n",
    "image = Image.open(r\"C:\\Users\\ASUS\\Desktop\\mask.jpg\")\n",
    "\n",
    "#resize the image to a 224x224 with the same strategy as in TM2:\n",
    "#resizing the image to be at least 224x224 and then cropping from the center\n",
    "size = (224, 224)\n",
    "image = ImageOps.fit(image, size, Image.ANTIALIAS)\n",
    "\n",
    "#turn the image into a numpy array\n",
    "image_array = np.asarray(image)\n",
    "\n",
    "# display the resized image\n",
    "image.show()\n",
    "\n",
    "# Normalize the image\n",
    "normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
    "\n",
    "# Load the image into the array\n",
    "data[0] = normalized_image_array\n",
    "\n",
    "# run the inference\n",
    "prediction = model.predict(data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a6a10dc4245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#turn the image into a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project\\lib\\site-packages\\PIL\\ImageOps.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(image, size, method, bleed, centering)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;31m# number of pixels to trim off on Top and Bottom, Left and Right\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m     \u001b[0mbleed_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbleed\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleed\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     live_size = (\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "#from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "# Load the model\n",
    "model =load_model(r\"C:\\Users\\ASUS\\Downloads\\converted_keras\\keras_model.h5\",compile=False)\n",
    "face_clsfr=cv2.CascadeClassifier(r\"E:\\image_data\\haarcascade_frontalface_alt.xml\")\n",
    "\n",
    "source=cv2.VideoCapture(0)\n",
    "\n",
    "labels_dict={0:'with_mask',1:'without_mask'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "\n",
    "while(True):\n",
    "\n",
    "    ret,image=source.read()\n",
    "    size = (224, 224)\n",
    "    image = ImageOps.fit(image, size, Image.ANTIALIAS)\n",
    "\n",
    "    #turn the image into a numpy array\n",
    "    image_array = np.asarray(image)\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_clsfr.detectMultiScale(gray,1.3,5)  \n",
    "\n",
    "    for x,y,w,h in faces:\n",
    "    \n",
    "        face_img=gray[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,100,100,1))\n",
    "        result=model.predict(reshaped)\n",
    "\n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(\n",
    "          img, labels_dict[label], \n",
    "          (x, y-10),\n",
    "          cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "        \n",
    "    cv2.imshow('LIVE',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
